// Basic functionality test script
import { searchKnowledge } from '../src/server/vector/search'
import { buildPrompt } from '../src/server/prompt/buildPrompt'
import { chatCompletion } from '../src/server/openai'
import { prisma } from '../src/server/db/client'

async function testBasicFunctionality() {
  console.log('ğŸ§ª ×‘×“×™×§×•×ª ×‘×¡×™×¡×™×•×ª ×©×œ ×”××¢×¨×›×ª\n')
  console.log('='.repeat(80))

  // Test 1: Database connection
  console.log('\nğŸ“Š ×‘×“×™×§×” 1: ×—×™×‘×•×¨ ×œ××¡×“ ×”× ×ª×•× ×™×...')
  try {
    await prisma.$connect()
    const count = await prisma.knowledgeChunk.count()
    console.log(`âœ… ×—×™×‘×•×¨ ×œ××¡×“ ×”× ×ª×•× ×™× ×”×¦×œ×™×— (${count} chunks ×‘××¡×“ ×”× ×ª×•× ×™×)`)
  } catch (error) {
    console.error('âŒ ×©×’×™××” ×‘×—×™×‘×•×¨ ×œ××¡×“ ×”× ×ª×•× ×™×:', error)
    process.exit(1)
  }

  // Test 2: RAG Search
  console.log('\nğŸ” ×‘×“×™×§×” 2: RAG Search...')
  try {
    const query = '××” ×–×” ××¢×’×œ ×”×ª×•×“×¢×”?'
    const results = await searchKnowledge(query, 3)
    console.log(`âœ… RAG Search ×”×¦×œ×™×— (× ××¦××• ${results.length} chunks)`)
    if (results.length > 0) {
      console.log(`   Chunk ×¨××©×•×Ÿ: ${results[0].id} (${results[0].text.substring(0, 60)}...)`)
    } else {
      console.log('   âš ï¸  ×œ× × ××¦××• chunks - ×™×™×ª×›×Ÿ ×©×¦×¨×™×š ×œ×¨×•×¥ index')
    }
  } catch (error) {
    console.error('âŒ ×©×’×™××” ×‘-RAG Search:', error)
    process.exit(1)
  }

  // Test 3: LLM Connection
  console.log('\nğŸ¤– ×‘×“×™×§×” 3: ×—×™×‘×•×¨ ×œ-LLM (Ollama)...')
  try {
    const testMessages = [
      {
        role: 'user' as const,
        content: '×ª×©×•×‘×” ×§×¦×¨×” ×‘×¢×‘×¨×™×ª: ××” ×–×” ××¢×’×œ ×”×ª×•×“×¢×”?'
      }
    ]
    const response = await chatCompletion(testMessages, {
      temperature: 0.3,
      maxTokens: 50
    })
    const answer = response.choices[0]?.message?.content || ''
    if (answer.length > 0) {
      console.log(`âœ… LLM ×¢×•×‘×“ (×ª×©×•×‘×”: ${answer.substring(0, 100)}...)`)
    } else {
      console.log('âš ï¸  LLM ×”×’×™×‘ ××‘×œ ×”×ª×©×•×‘×” ×¨×™×§×”')
    }
  } catch (error) {
    console.error('âŒ ×©×’×™××” ×‘×—×™×‘×•×¨ ×œ-LLM:', error)
    console.error('   ×•×“× ×©-Ollama ×¨×¥: ollama serve')
    process.exit(1)
  }

  // Test 4: Full RAG + LLM
  console.log('\nğŸ”„ ×‘×“×™×§×” 4: RAG + LLM (×ª×©×•×‘×” ××œ××”)...')
  try {
    const query = '××” ×–×” ××¢×’×œ ×”×ª×•×“×¢×”?'
    const knowledgeChunks = await searchKnowledge(query, 3)
    const promptMessages = buildPrompt(query, [], knowledgeChunks, [])
    
    const response = await chatCompletion(promptMessages, {
      temperature: 0.3,
      maxTokens: 200
    })
    const answer = response.choices[0]?.message?.content || ''
    
    if (answer.length > 0) {
      console.log(`âœ… ×ª×©×•×‘×” ××œ××” ×”×ª×§×‘×œ×” (${answer.length} ×ª×•×•×™×)`)
      console.log(`\nğŸ“ ×ª×©×•×‘×”:`)
      console.log(`   ${answer.substring(0, 200)}${answer.length > 200 ? '...' : ''}`)
    } else {
      console.log('âš ï¸  ×ª×©×•×‘×” ×¨×™×§×”')
    }
  } catch (error) {
    console.error('âŒ ×©×’×™××” ×‘×ª×©×•×‘×” ××œ××”:', error)
    process.exit(1)
  }

  console.log('\n' + '='.repeat(80))
  console.log('âœ… ×›×œ ×”×‘×“×™×§×•×ª ×”×•×©×œ××• ×‘×”×¦×œ×—×”!')
  console.log('='.repeat(80))

  await prisma.$disconnect()
}

testBasicFunctionality().catch(console.error)

