// Test script for "××” ×–×” ××¢×’×œ ×ª×•×“×¢×”?" query
import { prisma } from '../src/server/db/client'
import { searchKnowledge } from '../src/server/vector/search'
import { buildPrompt } from '../src/server/prompt/buildPrompt'
import { chatCompletion } from '../src/server/openai'

async function testCircleQuery() {
  const testQuery = '××” ×–×” ××¢×’×œ ×ª×•×“×¢×”?'
  
  console.log('ğŸ§ª Testing query:', testQuery)
  console.log('='.repeat(80))
  
  try {
    // 1. Search RAG
    console.log('\nğŸ“š Step 1: RAG Search')
    const knowledgeChunks = await searchKnowledge(testQuery, 5)
    
    console.log(`\nâœ… Retrieved ${knowledgeChunks.length} chunks:`)
    knowledgeChunks.forEach((chunk, idx) => {
      const title = chunk.metadata?.title || chunk.id
      const order = chunk.metadata?.order ?? 'N/A'
      console.log(`\n  [${idx + 1}] ID: ${chunk.id}`)
      console.log(`      Title: ${title}`)
      console.log(`      Order: ${order}`)
      console.log(`      Text: ${chunk.text.substring(0, 300)}...`)
      
      // Check if contains the term
      if (chunk.text.includes('××¢×’×œ ×”×ª×•×“×¢×”') || chunk.text.includes('××¢×’×œ ×ª×•×“×¢×”')) {
        console.log(`      âœ… CONTAINS "××¢×’×œ ×”×ª×•×“×¢×”"`)
      }
    })
    
    // 2. Check if relevant chunks were found
    const hasRelevantContent = knowledgeChunks.some(chunk => 
      chunk.text.includes('××¢×’×œ ×”×ª×•×“×¢×”') || 
      chunk.text.includes('××¢×’×œ ×ª×•×“×¢×”')
    )
    
    console.log(`\nğŸ“Š Analysis:`)
    console.log(`   Has relevant content: ${hasRelevantContent ? 'âœ… YES' : 'âŒ NO'}`)
    
    if (!hasRelevantContent) {
      console.log(`   âš ï¸  WARNING: No chunks found containing "××¢×’×œ ×”×ª×•×“×¢×”"`)
      console.log(`   This may indicate the RAG search needs improvement or chunks need re-indexing`)
    }
    
    // 3. Build prompt
    console.log('\nğŸ“ Step 2: Building Prompt')
    const promptMessages = buildPrompt(
      testQuery,
      [], // No conversation history
      knowledgeChunks,
      [] // No user memories
    )
    
    // Extract and display CONTEXT
    const contextMessage = promptMessages.find(m => {
      const content = typeof m.content === 'string' ? m.content : ''
      return content.includes('CONTEXT (from course materials)')
    })
    
    if (contextMessage) {
      const contextText = typeof contextMessage.content === 'string' ? contextMessage.content : ''
      console.log(`\nâœ… CONTEXT block found (${contextText.length} chars):`)
      console.log(contextText)
      console.log('\n' + '='.repeat(80))
    } else {
      console.log(`\nâŒ No CONTEXT block found in prompt!`)
    }
    
    // 4. Show system message
    const systemMessage = promptMessages.find(m => m.role === 'system' && 
      typeof m.content === 'string' && 
      m.content.includes('CONTEXT USAGE RULES')
    )
    
    if (systemMessage) {
      const systemText = typeof systemMessage.content === 'string' ? systemMessage.content : ''
      console.log(`\nâœ… System message contains context rules:`)
      if (systemText.includes('×× ×™ ×œ× ×¨×•××” ×”×¡×‘×¨ ×‘×¨×•×¨')) {
        console.log(`   âœ… Contains correct Hebrew fallback message`)
      }
      if (systemText.includes('Do NOT invent generic psychological explanations')) {
        console.log(`   âœ… Contains anti-hallucination rule`)
      }
    }
    
    // 5. Test LLM call (only if we have context)
    if (hasRelevantContent) {
      console.log('\nğŸ¤– Step 3: Testing LLM Response')
      console.log('Calling local LLM (Ollama)...\n')
      
      const completion = await chatCompletion(promptMessages, {
        temperature: 0.7,
        maxTokens: 500,
      })
      
      const response = completion.choices[0]?.message?.content?.trim() || ''
      
      console.log('Response:')
      console.log(response)
      console.log('\n' + '='.repeat(80))
      
      // Verify response
      if (response.includes('××¢×’×œ ×”×ª×•×“×¢×”') || response.includes('××¢×’×œ ×ª×•×“×¢×”')) {
        console.log('âœ… Response mentions "××¢×’×œ ×”×ª×•×“×¢×”"')
      }
      
      if (response.includes('××¡××š') || response.includes('××œ×') || response.includes('×©×™×¢×•×¨')) {
        console.log('âœ… Response appears to use course material definition')
      }
      
      if (response.includes('××—×©×‘×•×ª') && response.includes('×¨×’×©×•×ª') && response.includes('×’×•×£')) {
        console.log('âš ï¸  WARNING: Response may contain generic CBT explanation instead of course material')
      }
      
      if (response.includes('×× ×™ ×œ× ×¨×•××” ×”×¡×‘×¨ ×‘×¨×•×¨')) {
        console.log('âœ… Response correctly says it cannot find explanation (if answer not in context)')
      }
    } else {
      console.log('\nâ­ï¸  Skipping LLM call (no relevant content or no API key)')
    }
    
    console.log('\nâœ… Test complete!')
    
  } catch (error) {
    console.error('âŒ Error:', error)
    if (error instanceof Error && error.message.includes('DATABASE_URL')) {
      console.error('\nğŸ’¡ Tip: Make sure DATABASE_URL is set in .env file')
    }
  } finally {
    await prisma.$disconnect()
  }
}

testCircleQuery().catch(console.error)

