#!/usr/bin/env python3
"""
Test script for llama.cpp integration
"""
import os
import sys

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Set to use llama.cpp
os.environ["USE_LLAMA_CPP"] = "true"

def test_llama_cpp():
    """Test llama.cpp integration"""
    print("ğŸ§ª Testing llama.cpp Integration")
    print("=" * 80)
    
    # Check if model path is set
    model_path = os.getenv("LLAMA_CPP_MODEL_PATH", "models/dictalm2.0-instruct.gguf")
    print(f"ğŸ“¦ Model path: {model_path}")
    
    if not os.path.exists(model_path):
        print(f"\nâš ï¸  Model file not found: {model_path}")
        print("\nğŸ’¡ To download a model, run:")
        print("   python3 scripts/download_gguf_model.py qwen2.5-7b")
        print("\n   Or set LLAMA_CPP_MODEL_PATH to point to your GGUF model")
        return False
    
    try:
        from rag.llama_cpp_llm import call_llm
        
        # Test with simple context
        context = [{
            'text': '××¢×’×œ ×”×ª×•×“×¢×” ×”×•× ×›×œ×™ ×›×ª×•×‘ ×©×××œ××™× ×”××©×ª×ª×¤×™× ×‘×¡×•×£ ×›×œ ×©×™×¢×•×¨. ×”×•× × ×•×¢×“ ×œ×©×§×£ ×ª×•×‘× ×•×ª, ×ª×¨×’×•×œ×™× ×•×§×©×™×™×.',
            'source': 'test.md',
            'chunk_index': 0
        }]
        
        question = '××” ×–×” ××¢×’×œ ×”×ª×•×“×¢×”?'
        print(f"\nâ“ ×©××œ×”: {question}")
        print("=" * 80)
        
        answer = call_llm(question, context, max_new_tokens=150)
        
        print(f"\nğŸ“£ ×ª×©×•×‘×”:")
        print(answer)
        print("=" * 80)
        
        # Check for non-Hebrew text
        import re
        non_hebrew = re.findall(r'[a-zA-Z]{3,}', answer)
        if non_hebrew:
            print(f"\nâš ï¸  × ××¦××• ××™×œ×™× ×‘×× ×’×œ×™×ª: {non_hebrew[:5]}")
        else:
            print("\nâœ… ×ª×©×•×‘×” ×‘×¢×‘×¨×™×ª ×‘×œ×‘×“!")
        
        return True
        
    except Exception as e:
        print(f"\nâŒ ×©×’×™××”: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = test_llama_cpp()
    if success:
        print("\nâœ… llama.cpp integration works!")
    else:
        print("\nâŒ llama.cpp integration failed - check model path")

