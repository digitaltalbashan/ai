#!/usr/bin/env python3
"""
×ª×•×›× ×™×ª ×‘×“×™×§×•×ª ××§×™×¤×” ×œ××¢×¨×›×ª RAG + Dicta-LM
×‘×•×“×§×ª 4 ×©×›×‘×•×ª: ×™×“×¢, ×©××œ×•×ª ××™×©×™×•×ª, ×¡××•×œ ×˜×•×§, ×•×¤×™×™×¤×œ×™×™×Ÿ
"""
import sys
import os
import json
import time
import argparse
from typing import Dict, List, Tuple, Optional
from datetime import datetime

# Add project root to path
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from rag.query_improved import RagQueryEngine, call_llm_default


class TestResult:
    """Represents a single test result"""
    def __init__(self, category: str, subcategory: str, question: str):
        self.category = category
        self.subcategory = subcategory
        self.question = question
        self.answer = ""
        self.chunks = []
        self.prompt = ""  # Will be filled if save_prompt=True
        self.timing = {}
        self.scores = {}
        self.notes = ""
        self.timestamp = datetime.now().isoformat()
    
    def to_dict(self):
        return {
            "category": self.category,
            "subcategory": self.subcategory,
            "question": self.question,
            "answer": self.answer,
            "chunks_count": len(self.chunks),
            "chunks": [
                {
                    "id": c.get("id", ""),
                    "source": c.get("source", ""),
                    "text_preview": c.get("text", "")[:200] + "...",
                    "text_full": c.get("text", ""),  # Full text for analysis
                    "rerank_score": c.get("rerank_score", 0),
                    "distance": c.get("distance", 0)
                }
                for c in self.chunks
            ],
            "prompt": self.prompt if hasattr(self, 'prompt') else "",
            "timing": self.timing,
            "scores": self.scores,
            "notes": self.notes,
            "timestamp": self.timestamp
        }


class ComprehensiveTestSuite:
    """Comprehensive test suite for RAG + LLM system"""
    
    def __init__(self, verbose: bool = True):
        self.verbose = verbose
        self.engine = RagQueryEngine()
        self.results: List[TestResult] = []
        self.conversation_context = {}  # For testing memory/context
        self.total_tests = 0
        self.current_test = 0
        
    def run_test(self, category: str, subcategory: str, question: str, 
                 context_key: Optional[str] = None, save_prompt: bool = False) -> TestResult:
        """Run a single test and return result"""
        self.current_test += 1
        result = TestResult(category, subcategory, question)
        
        print(f"\n{'='*80}")
        if self.total_tests > 0:
            print(f"ğŸ“Š ×”×ª×§×“××•×ª: {self.current_test}/{self.total_tests} ({self.current_test*100//self.total_tests}%)")
        print(f"ğŸ“‹ {category} > {subcategory}")
        print(f"â“ ×©××œ×”: {question}")
        print(f"{'='*80}")
        
        # Add conversation context if provided
        full_question = question
        if context_key and context_key in self.conversation_context:
            full_question = f"{self.conversation_context[context_key]}\n\n{question}"
            if self.verbose:
                print(f"ğŸ’­ ××©×ª××© ×‘×”×§×©×¨ ×©×™×—×” ×§×•×“× ({len(self.conversation_context[context_key])} ×ª×•×•×™×)")
        
        # Step 1: Retrieve chunks
        if self.verbose:
            print(f"\nğŸ” ×©×œ×‘ 1/3: ×—×™×¤×•×© chunks ×‘×××’×¨ ×”×™×“×¢...")
        retrieve_start = time.time()
        
        # Get answer with timing
        start_time = time.time()
        answer, sources, timing_info = self.engine.answer(
            full_question,
            llm_callable=call_llm_default,
            measure_time=True
        )
        total_time = time.time() - start_time
        
        if self.verbose:
            retrieve_time = timing_info.get("retrieve_time", 0) if timing_info else 0
            rerank_time = timing_info.get("rerank_time", 0) if timing_info else 0
            llm_time = timing_info.get("llm_time", 0) if timing_info else 0
            print(f"   âœ… × ××¦××• {len(sources)} chunks ({retrieve_time:.2f}s)")
            print(f"   ğŸ”„ Rerank: {rerank_time:.2f}s")
            print(f"   ğŸ¤– LLM: {llm_time:.2f}s")
            print(f"   â±ï¸  ×¡×”\"×›: {total_time:.2f}s")
        
        result.answer = answer
        result.chunks = sources
        
        # Try to capture prompt if save_prompt is True
        if save_prompt:
            try:
                # Import build_prompt to reconstruct it
                from rag.llama_cpp_llm import build_prompt
                prompt_text = build_prompt(full_question, sources)
                result.prompt = prompt_text
            except:
                result.prompt = "×œ× × ×™×ª×Ÿ ×œ×©×—×–×¨ prompt"
        
        result.timing = {
            "retrieve_time": timing_info.get("retrieve_time", 0) if timing_info else 0,
            "rerank_time": timing_info.get("rerank_time", 0) if timing_info else 0,
            "llm_time": timing_info.get("llm_time", 0) if timing_info else 0,
            "total_time": timing_info.get("total_time", total_time) if timing_info else total_time
        }
        
        # Analyze result
        self._analyze_result(result, category, subcategory)
        
        # Display result
        print(f"\n{'='*80}")
        print(f"ğŸ“£ ×ª×•×¦××•×ª:")
        print(f"{'='*80}")
        print(f"\nğŸ’¬ ×ª×©×•×‘×” ({len(answer)} ×ª×•×•×™×):")
        print(f"{'-'*80}")
        print(answer)
        print(f"{'-'*80}")
        
        print(f"\nğŸ“Š ×¡×˜×˜×™×¡×˜×™×§×•×ª:")
        print(f"   â€¢ Chunks ×©× ××¦××•: {len(sources)}")
        print(f"   â€¢ ×–××Ÿ ×›×•×œ×œ: {result.timing['total_time']:.2f}s")
        if timing_info:
            print(f"   â€¢ ×–××Ÿ ×—×™×¤×•×©: {result.timing.get('retrieve_time', 0):.2f}s")
            print(f"   â€¢ ×–××Ÿ rerank: {result.timing.get('rerank_time', 0):.2f}s")
            print(f"   â€¢ ×–××Ÿ LLM: {result.timing.get('llm_time', 0):.2f}s")
        
        if result.scores:
            print(f"\nğŸ“ˆ ×¦×™×•× ×™×:")
            for key, value in result.scores.items():
                print(f"   â€¢ {key}: {value}/5")
        
        if sources:
            print(f"\nğŸ“š Top 3 Chunks:")
            for i, chunk in enumerate(sources[:3], 1):
                print(f"   [{i}] {chunk.get('source', 'unknown')}")
                print(f"       Rerank: {chunk.get('rerank_score', 0):.3f} | Distance: {chunk.get('distance', 0):.3f}")
                print(f"       Preview: {chunk.get('text', '')[:150]}...")
        
        self.results.append(result)
        return result
    
    def _analyze_result(self, result: TestResult, category: str, subcategory: str):
        """Analyze test result and assign scores"""
        answer = result.answer.lower()
        chunks_text = " ".join([c.get("text", "").lower() for c in result.chunks])
        
        # Category-specific analysis
        if category == "×™×“×¢":
            if subcategory == "××•×©×’×™× ××¨×›×–×™×™×":
                # Check for key concepts
                key_concepts = ["×¨×™××§×˜×™×‘×™", "×§×¨×™××˜×™×‘×™", "××§×˜×™×‘×™", "××¡×›×”", "×›×•×‘×¢", 
                               "××” ×©××•××¨ ×¦×•××—", "×××‘×˜×™×” ×¨×’×©×™×ª"]
                found_concepts = [c for c in key_concepts if c in answer]
                result.scores["concepts_found"] = len(found_concepts)
                result.scores["concepts_total"] = len(key_concepts)
                
                # Check for accuracy (no mixing concepts)
                if "××¡×›×”" in answer and "×›×•×‘×¢" in answer:
                    if "×¤×—×“" in answer or "×“×—×™×™×”" in answer:
                        result.scores["accuracy"] = 5
                    else:
                        result.scores["accuracy"] = 3
                else:
                    result.scores["accuracy"] = 2
                
            elif subcategory == "×™×™×©×•×":
                # Check for empathy first
                empathy_indicators = ["×©×•××¢", "××‘×™×Ÿ", "×§×©×”", "×¨×’×©", "×ª×—×•×©×”"]
                has_empathy = any(ind in answer for ind in empathy_indicators)
                result.scores["empathy_first"] = 5 if has_empathy else 2
                
                # Check for root of action
                root_indicators = ["×©×•×¨×©", "×¤×—×“", "×¨×¦×•×Ÿ", "×—×•×¤×©×™", "×¨×™××§×˜×™×‘×™"]
                has_root = any(ind in answer for ind in root_indicators)
                result.scores["root_analysis"] = 5 if has_root else 2
                
            elif subcategory == "×××™× ×•×ª":
                # Check if model admits uncertainty
                uncertainty_indicators = ["×œ× ×¨×•××”", "×œ× × ××¦×", "×œ× ×›×ª×•×‘", "×œ× ×‘×˜×•×—", 
                                         "×œ×¤×™ ×¨×•×—", "×œ× ×¦×™×˜×•×˜"]
                has_uncertainty = any(ind in answer for ind in uncertainty_indicators)
                result.scores["honesty"] = 5 if has_uncertainty else 1
        
        elif category == "×©××œ×•×ª ××™×©×™×•×ª":
            # Check for empathy
            empathy_indicators = ["×©×•××¢", "××‘×™×Ÿ", "×§×©×”", "×¨×’×©", "×ª×—×•×©×”", "×× ×™ ×©× ×œ×‘"]
            has_empathy = any(ind in answer for ind in empathy_indicators)
            result.scores["empathy"] = 5 if has_empathy else 2
            
            # Check for open questions
            question_indicators = ["?", "××”", "××™×š", "×œ××”", "××™×–×”"]
            has_questions = any(ind in answer for ind in question_indicators)
            result.scores["open_questions"] = 5 if has_questions else 2
            
            # Check for context memory (if context_key provided)
            if result.question and "××‘×" in result.question or "×‘× ×•×ª" in result.question:
                if "××‘×" in answer or "×‘× ×•×ª" in answer:
                    result.scores["context_memory"] = 5
                else:
                    result.scores["context_memory"] = 2
        
        elif category == "×¡××•×œ ×˜×•×§":
            # Check for legitimacy
            legitimacy_indicators = ["×–×” ×‘×¡×“×¨", "××•×ª×¨", "×œ×’×™×˜×™××™", "××™×Ÿ ×‘×¢×™×”", "×–×” ×‘×¡×“×¨ ×’××•×¨"]
            has_legitimacy = any(ind in answer for ind in legitimacy_indicators)
            result.scores["legitimacy"] = 5 if has_legitimacy else 2
            
            # Check for soft ending
            soft_ending_indicators = ["?", "×ª×¨××”", "××™×–×”", "××” ×§×•×¨×”"]
            has_soft_ending = any(ind in answer[-100:] for ind in soft_ending_indicators)
            result.scores["soft_ending"] = 5 if has_soft_ending else 2
        
        # Pipeline analysis (for all categories)
        # Check chunk relevance
        if result.chunks:
            avg_rerank = sum(c.get("rerank_score", 0) for c in result.chunks) / len(result.chunks)
            result.scores["chunk_relevance"] = min(5, int(avg_rerank * 0.5))  # Scale to 1-5
            
            # Check if answer uses chunks
            chunk_keywords = set()
            for chunk in result.chunks[:3]:  # Top 3 chunks
                text = chunk.get("text", "").lower()
                # Extract key phrases (simple heuristic)
                words = text.split()[:20]  # First 20 words
                chunk_keywords.update(words)
            
            answer_words = set(answer.lower().split())
            overlap = len(chunk_keywords.intersection(answer_words))
            result.scores["chunk_usage"] = min(5, overlap // 5)  # Rough heuristic
        
        # Style check (for all categories)
        # Check for first person
        first_person = "×›×©×× ×™" in answer or "×× ×™ ×©×" in answer or "×× ×™ ×©×•××¢" in answer
        result.scores["first_person"] = 5 if first_person else 2
        
        # Check for soft language
        soft_indicators = ["×¨×š", "×¢×“×™×Ÿ", "×‘×¢×“×™× ×•×ª", "×‘×¨×›×•×ª", "×©×§×˜"]
        has_soft = any(ind in answer for ind in soft_indicators)
        result.scores["soft_language"] = 5 if has_soft else 3
        
        # Check for ending question/invitation
        ending_question = "?" in answer[-50:] or "×ª×¨××”" in answer[-50:] or "××™×–×”" in answer[-50:]
        result.scores["ending_invitation"] = 5 if ending_question else 2
    
    def run_knowledge_tests(self):
        """Run knowledge-based tests"""
        print("\n" + "="*80)
        print("ğŸ“š ×©×›×‘×” 1: ×‘×“×™×§×•×ª ×™×“×¢ (RAG / ×ª×•×›×Ÿ IMPACT)")
        print("="*80)
        
        # 2.1. ××•×©×’×™× ××¨×›×–×™×™×
        print("\nğŸ“– 2.1. ×©××œ×•×ª ×‘×¡×™×¡ ×¢×œ ××•×©×’×™× ××¨×›×–×™×™×")
        self.run_test("×™×“×¢", "××•×©×’×™× ××¨×›×–×™×™×", 
                     "×ª×¡×‘×™×¨ ×œ×™ ×‘×§×¦×¨×” ××” ×”×”×‘×“×œ ×‘×™×Ÿ ×ª×•×“×¢×” ×¨×™××§×˜×™×‘×™×ª ×œ×ª×•×“×¢×” ×§×¨×™××˜×™×‘×™×ª ×œ×¤×™ ×˜×œ ×‘×©×Ÿ.")
        self.run_test("×™×“×¢", "××•×©×’×™× ××¨×›×–×™×™×", 
                     "××” ×–××ª ××•××¨×ª '××” ×©××•××¨ ×¦×•××—' ×‘×”×•×¨×•×ª?")
        self.run_test("×™×“×¢", "××•×©×’×™× ××¨×›×–×™×™×", 
                     "××” ×”×”×‘×“×œ ×‘×™×Ÿ ××¡×›×” ×œ×›×•×‘×¢ ×‘×©×¤×” ×©×œ ×˜×œ?")
        self.run_test("×™×“×¢", "××•×©×’×™× ××¨×›×–×™×™×", 
                     "××” ×–×” '×××‘×˜×™×” ×¨×’×©×™×ª' ×•××™×š ×¢×•×©×™× ××ª ×–×” ×œ×‘×“ ×‘×‘×™×ª?")
        
        # 2.2. ×™×™×©×•×
        print("\nğŸ”§ 2.2. ×©××œ×•×ª ×™×™×©×•× (×™×“×¢ â†’ ×¤×¨×§×˜×™×§×”)")
        self.run_test("×™×“×¢", "×™×™×©×•×", 
                     "×”×‘×Ÿ ×©×œ×™ ×‘×Ÿ 6 ××•××¨ '×× ×™ ××¤×¡', ××™×š ×œ×¤×™ IMPACT × ×›×•×Ÿ ×œ×”×’×™×‘?")
        self.run_test("×™×“×¢", "×™×™×©×•×", 
                     "×× ×™ ××¨×’×™×© ×©×—×•×§ ×‘×¢×‘×•×“×”, ××” ×–×” ×™×›×•×œ ×œ×”×’×™×“ ×¢×œ ×”×¨×¦×•×Ÿ ×©×œ×™ ×œ×¤×™ ×˜×œ ×‘×©×Ÿ?")
        self.run_test("×™×“×¢", "×™×™×©×•×", 
                     "×™×© ×œ×™ ×§×•×©×™ ×œ×©×™× ×’×‘×•×œ ×‘×¢×‘×•×“×”, ××™×š ×¢×•×‘×¨×™× ××’×‘×•×œ ×¨×™××§×˜×™×‘×™ ×œ×¤×¨×•××§×˜×™×‘×™?")
        
        # 2.3. ×××™× ×•×ª
        print("\nğŸ” 2.3. ×©××œ×•×ª ×××™× ×•×ª / '×—×•×¨×™× ×‘×™×“×¢'")
        self.run_test("×™×“×¢", "×××™× ×•×ª", 
                     "×ª×Ÿ ×œ×™ ×¦×™×˜×•×˜ ××“×•×™×§ ×©×œ ×˜×œ ×‘×©×Ÿ ×¢×œ × ×•×©× '×ª×•×“×¢×” ×§×•×•× ×˜×™×ª'")
        self.run_test("×™×“×¢", "×××™× ×•×ª", 
                     "×‘××™×–×” ×¤×¨×§ ×˜×œ ××“×‘×¨ ×¢×œ '× ×™×”×•×œ ×–××Ÿ'?")
    
    def run_personal_tests(self):
        """Run personal/contextual tests"""
        print("\n" + "="*80)
        print("ğŸ‘¤ ×©×›×‘×” 2: ×©××œ×•×ª ××™×©×™×•×ª (×›×“×™ ×œ×”×‘×™×Ÿ ×”×§×©×¨)")
        print("="*80)
        
        # 3.1. ×¤×ª×™×—×” ××™×©×™×ª â†’ ×›××” ×¦×¢×“×™× ×§×“×™××”
        print("\nğŸ’¬ 3.1. ×¤×ª×™×—×” ××™×©×™×ª ××—×ª â†’ ×›××” ×¦×¢×“×™× ×§×“×™××”")
        
        context_key = "personal_context_1"
        self.conversation_context[context_key] = "×× ×™ ×‘×ª×§×•×¤×” ×××•×“ ×¢××•×¡×”, ××¨×’×™×© ×©×”×›×œ ×¢×œ×™×™, ×œ× ×™×•×“×¢ ×××™×¤×” ×œ×”×ª×—×™×œ."
        
        result1 = self.run_test("×©××œ×•×ª ××™×©×™×•×ª", "×¤×ª×™×—×” ××™×©×™×ª", 
                                "×× ×™ ×‘×ª×§×•×¤×” ×××•×“ ×¢××•×¡×”, ××¨×’×™×© ×©×”×›×œ ×¢×œ×™×™, ×œ× ×™×•×“×¢ ×××™×¤×” ×œ×”×ª×—×™×œ.",
                                context_key)
        
        # Continue conversation
        self.conversation_context[context_key] += f"\n\n×ª×©×•×‘×” ×§×•×“××ª: {result1.answer}"
        result2 = self.run_test("×©××œ×•×ª ××™×©×™×•×ª", "×¤×ª×™×—×” ××™×©×™×ª", 
                                "×× ×™ ×’× ×©× ×œ×‘ ×©×× ×™ ××ª×‘×™×™×© ×œ×‘×§×© ×¢×–×¨×”.",
                                context_key)
        
        self.conversation_context[context_key] += f"\n\n×ª×©×•×‘×” ×§×•×“××ª: {result2.answer}"
        result3 = self.run_test("×©××œ×•×ª ××™×©×™×•×ª", "×¤×ª×™×—×” ××™×©×™×ª", 
                                "×–×” ×§×©×•×¨ ××•×œ×™ ×œ×™×œ×“×•×ª ×©×œ×™, ×©×”×™×™×ª×™ '×”×—×–×§ ×‘×‘×™×ª'.",
                                context_key)
        
        # 3.2. ×‘×“×™×§×ª ×–×™×›×¨×•×Ÿ ×”×§×©×¨
        print("\nğŸ§  3.2. ×‘×“×™×§×ª ×–×™×›×¨×•×Ÿ ×”×§×©×¨")
        
        context_key2 = "parent_context"
        self.conversation_context[context_key2] = "×× ×™ ××‘× ×œ×©×ª×™ ×‘× ×•×ª, ××¨×’×™×© ×©××¤×¡×¤×¡ ××•×ª×Ÿ."
        
        result4 = self.run_test("×©××œ×•×ª ××™×©×™×•×ª", "×–×™×›×¨×•×Ÿ ×”×§×©×¨", 
                                "×× ×™ ××‘× ×œ×©×ª×™ ×‘× ×•×ª, ××¨×’×™×© ×©××¤×¡×¤×¡ ××•×ª×Ÿ.",
                                context_key2)
        
        # Later in conversation
        self.conversation_context[context_key2] += f"\n\n×ª×©×•×‘×” ×§×•×“××ª: {result4.answer}"
        result5 = self.run_test("×©××œ×•×ª ××™×©×™×•×ª", "×–×™×›×¨×•×Ÿ ×”×§×©×¨", 
                                "××™×š ××ª×” ×”×™×™×ª ××¡×‘×™×¨ ××ª ××” ×©×§×•×¨×” ×œ×™ ×›×”×•×¨×”?",
                                context_key2)
    
    def run_small_talk_tests(self):
        """Run small talk / humanity tests"""
        print("\n" + "="*80)
        print("ğŸ’­ ×©×›×‘×” 3: ×¡××•×œ ×˜×•×§ / ×× ×•×©×™×•×ª")
        print("="*80)
        
        self.run_test("×¡××•×œ ×˜×•×§", "×œ×’×™×˜×™××¦×™×”", 
                     "××©×¢×× ×œ×™ ×¢×›×©×™×•, ×‘× ×œ×™ ×¡×ª× ×œ×“×‘×¨.")
        self.run_test("×¡××•×œ ×˜×•×§", "×”×•××•×¨", 
                     "×¡×¤×¨ ×œ×™ ××©×”×• ××¦×—×™×§ ×¢×œ ×ª×•×“×¢×”.")
        self.run_test("×¡××•×œ ×˜×•×§", "×—×•×¡×¨ ×—×©×§", 
                     "××™×Ÿ ×œ×™ ×›×•×— ×¢×›×©×™×• ×œ'×¢×‘×•×“×” ×¢×¦××™×ª', ××¤×©×¨ ×¨×§ ×œ×“×‘×¨ ×¢×œ ×—×ª×•×œ×™×?")
    
    def run_pipeline_tests(self):
        """Run pipeline analysis tests"""
        print("\n" + "="*80)
        print("ğŸ”§ ×©×›×‘×” 4: ×‘×“×™×§×ª ×”×¤×™×™×¤×œ×™×™×Ÿ (××”-prompt â†’ chunks â†’ ×ª×©×•×‘×”)")
        print("="*80)
        
        # Run a test and analyze pipeline
        result = self.run_test("×¤×™×™×¤×œ×™×™×Ÿ", "× ×™×ª×•×— ××œ×", 
                              "××” ×–×” ××¢×’×œ ×”×ª×•×“×¢×”?",
                              save_prompt=True)
        
        # Detailed pipeline analysis
        print(f"\nğŸ“Š × ×™×ª×•×— ×¤×™×™×¤×œ×™×™×Ÿ ××¤×•×¨×˜:")
        print(f"   Chunks ×©× ×©×œ×¤×•: {len(result.chunks)}")
        for i, chunk in enumerate(result.chunks[:5], 1):
            print(f"   [{i}] {chunk.get('source', 'unknown')}")
            print(f"       Rerank Score: {chunk.get('rerank_score', 0):.3f}")
            print(f"       Distance: {chunk.get('distance', 0):.3f}")
            print(f"       Preview: {chunk.get('text', '')[:100]}...")
        
        if result.prompt:
            print(f"\n   ğŸ“ Prompt ({len(result.prompt)} ×ª×•×•×™×):")
            print(f"   {result.prompt[:300]}...")
        
        print(f"\n   ×ª×©×•×‘×” ({len(result.answer)} ×ª×•×•×™×)")
        print(f"   ×¦×™×•× ×™×: {result.scores}")
    
    def run_full_conversation_test(self):
        """Run a full conversation combining all layers"""
        print("\n" + "="*80)
        print("ğŸ¯ ×©×™×—×” ××œ××”: ×©×™×œ×•×‘ ×›×œ ×”×©×›×‘×•×ª")
        print("="*80)
        
        context_key = "full_conversation"
        self.conversation_context[context_key] = ""
        
        # 1. Knowledge question
        result1 = self.run_test("×©×™×—×” ××œ××”", "×™×“×¢", 
                               "××” ×–×” ×ª×•×“×¢×” ×¨×™××§×˜×™×‘×™×ª ×œ×¤×™ ×˜×œ ×‘×©×Ÿ?",
                               context_key)
        self.conversation_context[context_key] += f"\n\n×ª×©×•×‘×”: {result1.answer}"
        
        # 2. Personal application
        result2 = self.run_test("×©×™×—×” ××œ××”", "×™×™×©×•× ××™×©×™", 
                               "× ×¨××” ×œ×™ ×©×× ×™ ×›×–×” ××•×œ ××— ×©×œ×™ â€“ ×× ×™ ××ª×¤×•×¦×¥ ×¢×œ×™×• ×‘×§×œ×•×ª.",
                               context_key)
        self.conversation_context[context_key] += f"\n\n×ª×©×•×‘×”: {result2.answer}"
        
        # 3. Small talk
        result3 = self.run_test("×©×™×—×” ××œ××”", "×¡××•×œ ×˜×•×§", 
                               "×˜×•×‘, ×¢×©×™×ª ×œ×™ ×—×©×§ ×œ×‘×¨×•×— ×¢×›×©×™×• ×œ× ×˜×¤×œ×™×§×¡ ğŸ˜‚",
                               context_key)
    
    def generate_report(self, output_file: str = "test_results.json"):
        """Generate comprehensive test report"""
        report = {
            "timestamp": datetime.now().isoformat(),
            "total_tests": len(self.results),
            "results": [r.to_dict() for r in self.results],
            "summary": self._generate_summary()
        }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        print(f"\nâœ… ×“×•×— × ×©××¨ ×‘-{output_file}")
        return report
    
    def _generate_summary(self) -> Dict:
        """Generate summary statistics"""
        summary = {
            "by_category": {},
            "average_scores": {},
            "average_timing": {}
        }
        
        # Group by category
        for result in self.results:
            cat = result.category
            if cat not in summary["by_category"]:
                summary["by_category"][cat] = {"count": 0, "scores": []}
            summary["by_category"][cat]["count"] += 1
            summary["by_category"][cat]["scores"].extend(result.scores.values())
        
        # Calculate averages
        all_scores = []
        all_timings = []
        for result in self.results:
            if result.scores:
                all_scores.extend(result.scores.values())
            if result.timing:
                all_timings.append(result.timing.get("total_time", 0))
        
        summary["average_scores"]["overall"] = sum(all_scores) / len(all_scores) if all_scores else 0
        summary["average_timing"]["overall"] = sum(all_timings) / len(all_timings) if all_timings else 0
        
        return summary
    
    def print_summary(self):
        """Print test summary"""
        print("\n" + "="*80)
        print("ğŸ“Š ×¡×™×›×•× ×‘×“×™×§×•×ª")
        print("="*80)
        
        print(f"\n×¡×”\"×› ×‘×“×™×§×•×ª: {len(self.results)}")
        
        # By category
        by_category = {}
        for result in self.results:
            cat = result.category
            if cat not in by_category:
                by_category[cat] = []
            by_category[cat].append(result)
        
        for cat, results in by_category.items():
            print(f"\n{cat}: {len(results)} ×‘×“×™×§×•×ª")
            avg_score = sum(sum(r.scores.values()) / len(r.scores) if r.scores else 0 
                          for r in results) / len(results)
            print(f"   ×¦×™×•×Ÿ ×××•×¦×¢: {avg_score:.2f}/5")
        
        # Timing
        avg_time = sum(r.timing.get("total_time", 0) for r in self.results) / len(self.results)
        print(f"\nâ±ï¸  ×–××Ÿ ×××•×¦×¢ ×œ×‘×“×™×§×”: {avg_time:.2f}s")
    
    def run_single_test(self, question: str, category: str = "×‘×“×™×§×” ×™×—×™×“×”", 
                       subcategory: str = "×“×•×’××”", save_prompt: bool = True):
        """Run a single test with a custom question"""
        print("ğŸš€ ×”×¨×¦×ª ×‘×“×™×§×” ×™×—×™×“×”")
        print("="*80)
        self.total_tests = 1
        self.current_test = 0
        return self.run_test(category, subcategory, question, context_key=None, save_prompt=save_prompt)
    
    def close(self):
        """Close database connection"""
        self.engine.close()


def main():
    """Run comprehensive test suite"""
    parser = argparse.ArgumentParser(description='×ª×•×›× ×™×ª ×‘×“×™×§×•×ª ××§×™×¤×” - RAG + Dicta-LM')
    parser.add_argument('--single', '-s', type=str, help='×”×¨×¥ ×‘×“×™×§×” ××—×ª ×¢× ×©××œ×” ××•×ª×××ª ××™×©×™×ª')
    parser.add_argument('--category', '-c', type=str, default='×‘×“×™×§×” ×™×—×™×“×”', help='×§×˜×’×•×¨×™×” ×œ×‘×“×™×§×” ×™×—×™×“×”')
    parser.add_argument('--quiet', '-q', action='store_true', help='×”×¦×’ ×¤×—×•×ª ×¤×œ×˜')
    parser.add_argument('--test-id', '-t', type=int, help='×”×¨×¥ ×‘×“×™×§×” ×¡×¤×¦×™×¤×™×ª ×œ×¤×™ ID (1-20)')
    
    args = parser.parse_args()
    
    suite = ComprehensiveTestSuite(verbose=not args.quiet)
    
    try:
        if args.single:
            # Run single test
            result = suite.run_single_test(args.single, category=args.category)
            print(f"\nâœ… ×‘×“×™×§×” ×”×•×©×œ××”!")
            print(f"\nğŸ“„ ×¡×™×›×•×:")
            print(f"   ×©××œ×”: {result.question}")
            print(f"   ×ª×©×•×‘×”: {result.answer[:200]}...")
            print(f"   ×–××Ÿ: {result.timing.get('total_time', 0):.2f}s")
            print(f"   chunks: {len(result.chunks)}")
            if result.scores:
                print(f"   ×¦×™×•× ×™×: {result.scores}")
            
        elif args.test_id:
            # Run specific test by ID
            test_questions = {
                1: ("×™×“×¢", "××•×©×’×™× ××¨×›×–×™×™×", "×ª×¡×‘×™×¨ ×œ×™ ×‘×§×¦×¨×” ××” ×”×”×‘×“×œ ×‘×™×Ÿ ×ª×•×“×¢×” ×¨×™××§×˜×™×‘×™×ª ×œ×ª×•×“×¢×” ×§×¨×™××˜×™×‘×™×ª ×œ×¤×™ ×˜×œ ×‘×©×Ÿ."),
                2: ("×™×“×¢", "××•×©×’×™× ××¨×›×–×™×™×", "××” ×–××ª ××•××¨×ª '××” ×©××•××¨ ×¦×•××—' ×‘×”×•×¨×•×ª?"),
                3: ("×™×“×¢", "××•×©×’×™× ××¨×›×–×™×™×", "××” ×”×”×‘×“×œ ×‘×™×Ÿ ××¡×›×” ×œ×›×•×‘×¢ ×‘×©×¤×” ×©×œ ×˜×œ?"),
                4: ("×™×“×¢", "×™×™×©×•×", "×”×‘×Ÿ ×©×œ×™ ×‘×Ÿ 6 ××•××¨ '×× ×™ ××¤×¡', ××™×š ×œ×¤×™ IMPACT × ×›×•×Ÿ ×œ×”×’×™×‘?"),
                5: ("×©××œ×•×ª ××™×©×™×•×ª", "×¤×ª×™×—×” ××™×©×™×ª", "×× ×™ ×‘×ª×§×•×¤×” ×××•×“ ×¢××•×¡×”, ××¨×’×™×© ×©×”×›×œ ×¢×œ×™×™, ×œ× ×™×•×“×¢ ×××™×¤×” ×œ×”×ª×—×™×œ."),
                6: ("×¡××•×œ ×˜×•×§", "×œ×’×™×˜×™××¦×™×”", "××©×¢×× ×œ×™ ×¢×›×©×™×•, ×‘× ×œ×™ ×¡×ª× ×œ×“×‘×¨."),
                7: ("×¤×™×™×¤×œ×™×™×Ÿ", "× ×™×ª×•×— ××œ×", "××” ×–×” ××¢×’×œ ×”×ª×•×“×¢×”?"),
            }
            
            if args.test_id in test_questions:
                cat, subcat, q = test_questions[args.test_id]
                result = suite.run_test(cat, subcat, q, save_prompt=True)
                print(f"\nâœ… ×‘×“×™×§×” #{args.test_id} ×”×•×©×œ××”!")
            else:
                print(f"âŒ ×œ× × ××¦××” ×‘×“×™×§×” ×¢× ID {args.test_id}")
                print(f"   ×‘×“×™×§×•×ª ×–××™× ×•×ª: {list(test_questions.keys())}")
        
        else:
            # Run all tests
            print("ğŸš€ ×ª×•×›× ×™×ª ×‘×“×™×§×•×ª ××§×™×¤×” - RAG + Dicta-LM")
            print("="*80)
            
            # Count total tests
            suite.total_tests = 20  # Approximate count
            suite.current_test = 0
            
            # Run all test categories
            suite.run_knowledge_tests()
            suite.run_personal_tests()
            suite.run_small_talk_tests()
            suite.run_pipeline_tests()
            suite.run_full_conversation_test()
            
            # Generate report
            suite.print_summary()
            suite.generate_report()
            print("\nâœ… ×›×œ ×”×‘×“×™×§×•×ª ×”×•×©×œ××•!")
        
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ×‘×“×™×§×•×ª ×”×•×¤×¡×§×• ×¢×œ ×™×“×™ ×”××©×ª××©")
    except Exception as e:
        print(f"\n\nâŒ ×©×’×™××”: {e}")
        import traceback
        traceback.print_exc()
    finally:
        suite.close()


if __name__ == "__main__":
    main()

