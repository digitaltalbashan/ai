# 📊 ניתוח ביצועים - Performance Analysis

## 🔍 מפת התהליך המלא (מהשאלה עד התשובה)

```
┌─────────────────────────────────────────────────────────────────┐
│ 1. טעינת מודלים (רק פעם ראשונה)                                │
│    ├─ Embedding Model: ~2.7s → נשמר ב-cache                     │
│    └─ Rerank Model: ~1.8s → נשמר ב-cache                       │
└─────────────────────────────────────────────────────────────────┘
                          ↓
┌─────────────────────────────────────────────────────────────────┐
│ 2. יצירת Embedding לשאלה                                        │
│    └─ ~0.07-0.12s ✅ (מהיר מאוד)                               │
└─────────────────────────────────────────────────────────────────┘
                          ↓
┌─────────────────────────────────────────────────────────────────┐
│ 3. חיפוש במסד הנתונים (PostgreSQL + pgvector)                  │
│    └─ ~0.04-0.85s ⚠️ (משתנה מאוד)                             │
└─────────────────────────────────────────────────────────────────┘
                          ↓
┌─────────────────────────────────────────────────────────────────┐
│ 4. Re-ranking (CrossEncoder)                                    │
│    └─ ~0.29-0.37s ✅ (מהיר)                                    │
└─────────────────────────────────────────────────────────────────┘
                          ↓
┌─────────────────────────────────────────────────────────────────┐
│ 5. בניית Prompt                                                 │
│    └─ <0.001s ✅ (מיידי)                                       │
└─────────────────────────────────────────────────────────────────┘
                          ↓
┌─────────────────────────────────────────────────────────────────┐
│ 6. LLM Generation (HebrewLlama)                                │
│    └─ ~1.5-2.0s 🔴 (צוואר הבקבוק העיקרי!)                     │
└─────────────────────────────────────────────────────────────────┘
```

## 🔴 צווארי הבקבוק (לאחר טעינת המודלים)

### 🥇 1. LLM Generation: **1.5s (53.6%)** - צוואר הבקבוק העיקרי

**פירוט:**
- המודל `HebrewLlama` איטי יחסית
- זמן כולל: ~1.5-2.0 שניות
- אחוז מהזמן הכולל: 53.6%

**המלצות לשיפור:**
- ✅ נסה מודל מהיר יותר (Qwen 2.5, Mistral 7B)
- ✅ ודא ש-`n_gpu_layers=30` (Metal acceleration פעיל)
- ✅ הקטן `max_tokens` אם התשובות ארוכות מדי (כרגע 512)
- ✅ בדוק אם אפשר להשתמש ב-`n_threads` גבוה יותר

### 🥈 2. חיפוש במסד הנתונים: **0.85s (30.6%)**

**פירוט:**
- זמן משתנה מאוד: 0.04s - 0.85s
- תלוי בגודל המסד הנתונים (כרגע: 8,126 chunks)
- האינדקס `ivfflat` כבר מותאם (lists=100)

**המלצות לשיפור:**
- ✅ האינדקס כבר מותאם (נבדק עם `scripts/optimize_database_index.py`)
- ✅ אפשר להקטין `top_k_retrieve` מ-50 ל-30-40 (פחות מועמדים = חיפוש מהיר יותר)
- ✅ בדוק אם יש צורך ב-`ANALYZE` נוסף על הטבלה

### 🥉 3. Re-ranking: **0.37s (13.4%)** - סביר

**פירוט:**
- זמן יציב: ~0.29-0.37s
- עובד על 50 מועמדים
- `batch_size=32` (מותאם)

**המלצות:**
- ✅ כבר מהיר יחסית - אין צורך בשינויים
- אפשר להשאיר כמו שזה

## ⏱️ זמנים מפורטים

### שאלה ראשונה (כולל טעינת מודלים):
```
1.1 טעינת Embedding Model:     2.7s (42.2%)
1.2 טעינת Rerank Model:        1.8s (27.2%)
6. LLM Generation:              1.5s (23.9%)
4. Re-ranking:                  0.3s (4.4%)
2. יצירת Embedding לשאלה:       0.1s (1.8%)
3. חיפוש במסד הנתונים:          0.04s (0.5%)
─────────────────────────────────────────────
סה"כ:                           6.5s
```

### שאלות נוספות (אחרי טעינת מודלים):
```
6. LLM Generation:              1.5s (53.6%) 🔴
3. חיפוש במסד הנתונים:          0.85s (30.6%) ⚠️
4. Re-ranking:                  0.37s (13.4%) ✅
2. יצירת Embedding לשאלה:       0.07s (2.5%) ✅
─────────────────────────────────────────────
סה"כ:                           2.8s
```

## 💡 המלצות לשיפור כולל

### 1. LLM Generation (הכי חשוב - 53.6% מהזמן)

**אפשרויות:**
- [ ] נסה מודל מהיר יותר:
  - Qwen 2.5 7B Instruct (טוב מאוד בעברית)
  - Mistral 7B Instruct (מהיר יותר מ-HebrewLlama)
- [ ] ודא ש-Metal acceleration פעיל:
  ```bash
  export LLAMA_CPP_N_GPU_LAYERS=30
  ```
- [ ] הקטן `max_tokens` אם התשובות ארוכות מדי:
  ```python
  max_new_tokens=256  # במקום 512
  ```
- [ ] בדוק אם אפשר להגדיל `n_threads`:
  ```bash
  export LLAMA_CPP_N_THREADS=8  # במקום 4
  ```

### 2. חיפוש במסד הנתונים (30.6% מהזמן)

**אפשרויות:**
- [x] האינדקס כבר מותאם (ivfflat עם lists=100)
- [ ] הקטן `top_k_retrieve` מ-50 ל-30-40:
  ```python
  DEFAULT_TOP_K_RETRIEVE = 30  # במקום 50
  ```
- [ ] בדוק אם יש צורך ב-`ANALYZE` נוסף (כבר רץ)

### 3. Re-ranking (13.4% מהזמן)

**אפשרויות:**
- ✅ כבר מהיר יחסית - אין צורך בשינויים

## 📈 תוצאות צפויות לאחר שיפורים

### תרחיש אופטימי (מודל מהיר יותר + הקטנת top_k):
```
6. LLM Generation:              ~0.8s (50%)  ← שיפור של 50%
3. חיפוש במסד הנתונים:          ~0.4s (25%)  ← שיפור של 50%
4. Re-ranking:                  ~0.2s (12%)  ← שיפור קל
2. יצירת Embedding לשאלה:       ~0.07s (4%)
─────────────────────────────────────────────
סה"כ:                           ~1.5s  ← שיפור של 46%!
```

## 🛠️ כלים לניטור ביצועים

### הרצת ניתוח ביצועים:
```bash
cd /Users/tzahimoyal/TalBashanAI
source venv/bin/activate
export USE_LLAMA_CPP=true
export LLAMA_CPP_MODEL_PATH=models/hebrewllama.Q5_K_M.gguf
export LLAMA_CPP_N_GPU_LAYERS=30
python3 scripts/profile_performance.py
```

### בדיקת אינדקס:
```bash
python3 scripts/optimize_database_index.py
```

## 📝 הערות

1. **טעינת מודלים**: זה קורה רק פעם ראשונה, אחרי זה המודלים נשמרים ב-cache
2. **חיפוש במסד הנתונים**: הזמן משתנה מאוד - תלוי בגודל המסד הנתונים ובאינדקס
3. **LLM Generation**: זה השלב הכי איטי - מומלץ להתמקד כאן לשיפור

## ✅ סיכום

**צוואר הבקבוק העיקרי:** LLM Generation (1.5s, 53.6%)

**זמן כולל לשאלה (אחרי טעינת מודלים):** ~2.8 שניות

**המלצה ראשית:** נסה מודל מהיר יותר (Qwen 2.5 או Mistral 7B) כדי להקטין את זמן ה-LLM Generation מ-1.5s ל-~0.8s, מה שיקטין את הזמן הכולל ל-~1.5 שניות (שיפור של 46%).

